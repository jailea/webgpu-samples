(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[77],{5458:function(e,n,t){"use strict";t.d(n,{Tl:function(){return makeSample},hu:function(){return assert}});var r=t(5893),a=t(9008),i=t.n(a),o=t(1163),s=t(7294),u=t(4131),l=t.n(u);t(6876);let SampleLayout=e=>{let n=(0,s.useRef)(null),a=(0,s.useMemo)(()=>e.sources.map(e=>{let{name:n,contents:a}=e;return{name:n,...function(e){let n;let a=null;{a=document.createElement("div");let e=t(4631);n=e(a,{lineNumbers:!0,lineWrapping:!0,theme:"monokai",readOnly:!0})}return{Container:function(t){return(0,r.jsx)("div",{...t,children:(0,r.jsx)("div",{ref:t=>{a&&t&&(t.appendChild(a),n.setOption("value",e))}})})}}}(a)}}),e.sources),u=(0,s.useRef)(null),c=(0,s.useMemo)(()=>{if(e.gui){let e=t(4376),n=new e.GUI({autoPlace:!1});return n.domElement.style.position="relative",n.domElement.style.zIndex="1000",n}},[]),d=(0,s.useRef)(null),f=(0,s.useMemo)(()=>{if(e.stats){let e=t(2792);return new e}},[]),m=(0,o.useRouter)(),p=m.asPath.match(/#([a-zA-Z0-9\.\/]+)/),[h,g]=(0,s.useState)(null),[v,x]=(0,s.useState)(null);return(0,s.useEffect)(()=>{if(p?x(p[1]):x(a[0].name),c&&u.current)for(u.current.appendChild(c.domElement);c.__controllers.length>0;)c.__controllers[0].remove();f&&d.current&&(f.dom.style.position="absolute",f.showPanel(1),d.current.appendChild(f.dom));let t={active:!0};try{let r=n.current;if(!r)throw Error("The canvas is not available");let a=e.init({canvas:r,pageState:t,gui:c,stats:f});a instanceof Promise&&a.catch(e=>{console.error(e),g(e)})}catch(e){console.error(e),g(e)}return()=>{t.active=!1}},[]),(0,r.jsxs)("main",{children:[(0,r.jsxs)(i(),{children:[(0,r.jsx)("style",{dangerouslySetInnerHTML:{__html:"\n            .CodeMirror {\n              height: auto !important;\n              margin: 1em 0;\n            }\n\n            .CodeMirror-scroll {\n              height: auto !important;\n              overflow: visible !important;\n            }\n          "}}),(0,r.jsx)("title",{children:"".concat(e.name," - WebGPU Samples")}),(0,r.jsx)("meta",{name:"description",content:e.description}),(0,r.jsx)("meta",{httpEquiv:"origin-trial",content:e.originTrial})]}),(0,r.jsxs)("div",{children:[(0,r.jsx)("h1",{children:e.name}),(0,r.jsx)("a",{target:"_blank",rel:"noreferrer",href:"https://github.com/".concat("Cryszzz/webgpu-samples","/tree/main/").concat(e.filename),children:"See it on Github!"}),(0,r.jsx)("p",{children:e.description}),h?(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)("p",{children:"Something went wrong. Do your browser and device support WebGPU?"}),(0,r.jsx)("p",{children:"".concat(h)})]}):null]}),(0,r.jsxs)("div",{className:l().canvasContainer,children:[(0,r.jsx)("div",{style:{position:"absolute",left:10},ref:d}),(0,r.jsx)("div",{style:{position:"absolute",right:10},ref:u}),(0,r.jsx)("canvas",{ref:n})]}),(0,r.jsxs)("div",{children:[(0,r.jsx)("nav",{className:l().sourceFileNav,children:(0,r.jsx)("ul",{children:a.map((e,n)=>(0,r.jsx)("li",{children:(0,r.jsx)("a",{href:"#".concat(e.name),"data-active":v==e.name,onClick:()=>{x(e.name)},children:e.name})},n))})}),a.map((e,n)=>(0,r.jsx)(e.Container,{className:l().sourceFileContainer,"data-active":v==e.name},n))]})]})},makeSample=e=>(0,r.jsx)(SampleLayout,{...e});function assert(e,n){if(!e)throw Error(n)}},1061:function(e,n,t){"use strict";t.d(n,{W:function(){return i}});var r=t(6906),a=t(2515);let i={positions:r.m,triangles:r.g,normals:[],uvs:[]};i.normals=(0,a.b)(i.positions,i.triangles),i.uvs=(0,a.q)(i.positions,"xy"),i.triangles.push([i.positions.length,i.positions.length+2,i.positions.length+1],[i.positions.length,i.positions.length+1,i.positions.length+3]),i.positions.push([-100,20,-100],[100,20,100],[-100,20,100],[100,20,-100]),i.normals.push([0,1,0],[0,1,0],[0,1,0],[0,1,0]),i.uvs.push([0,0],[1,1],[0,1],[1,0])},2515:function(e,n,t){"use strict";t.d(n,{b:function(){return computeSurfaceNormals},q:function(){return computeProjectedPlaneUVs}});var r=t(6416);function computeSurfaceNormals(e,n){let t=e.map(()=>[0,0,0]);return n.forEach(n=>{let[a,i,o]=n,s=e[a],u=e[i],l=e[o],c=r.R3.subtract(u,s),d=r.R3.subtract(l,s);r.R3.normalize(c,c),r.R3.normalize(d,d);let f=r.R3.cross(c,d);r.R3.add(t[a],f,t[a]),r.R3.add(t[i],f,t[i]),r.R3.add(t[o],f,t[o])}),t.forEach(e=>{r.R3.normalize(e,e)}),t}let a={xy:[0,1],xz:[0,2],yz:[1,2]};function computeProjectedPlaneUVs(e){let n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"xy",t=a[n],r=e.map(()=>[0,0]),i=[1/0,1/0],o=[-1/0,-1/0];return e.forEach((e,n)=>{r[n][0]=e[t[0]],r[n][1]=e[t[1]],i[0]=Math.min(e[t[0]],i[0]),i[1]=Math.min(e[t[1]],i[1]),o[0]=Math.max(e[t[0]],o[0]),o[1]=Math.max(e[t[1]],o[1])}),r.forEach(e=>{e[0]=(e[0]-i[0])/(o[0]-i[0]),e[1]=(e[1]-i[1])/(o[1]-i[1])}),r}},2077:function(e,n,t){"use strict";t.r(n),t.d(n,{default:function(){return main}});var r=t(6416),a=t(5458),i=t(1061),o="struct Scene {\n  lightViewProjMatrix: mat4x4<f32>,\n  cameraViewProjMatrix: mat4x4<f32>,\n  lightPos: vec3<f32>,\n}\n\nstruct Model {\n  modelMatrix: mat4x4<f32>,\n}\n\n@group(0) @binding(0) var<uniform> scene : Scene;\n@group(1) @binding(0) var<uniform> model : Model;\n\n@vertex\nfn main(\n  @location(0) position: vec3<f32>\n) -> @builtin(position) vec4<f32> {\n  return scene.lightViewProjMatrix * model.modelMatrix * vec4(position, 1.0);\n}\n",s="struct Scene {\n  lightViewProjMatrix: mat4x4<f32>,\n  cameraViewProjMatrix: mat4x4<f32>,\n  lightPos: vec3<f32>,\n}\n\nstruct Model {\n  modelMatrix: mat4x4<f32>,\n}\n\n@group(0) @binding(0) var<uniform> scene : Scene;\n@group(1) @binding(0) var<uniform> model : Model;\n\nstruct VertexOutput {\n  @location(0) shadowPos: vec3<f32>,\n  @location(1) fragPos: vec3<f32>,\n  @location(2) fragNorm: vec3<f32>,\n\n  @builtin(position) Position: vec4<f32>,\n}\n\n@vertex\nfn main(\n  @location(0) position: vec3<f32>,\n  @location(1) normal: vec3<f32>\n) -> VertexOutput {\n  var output : VertexOutput;\n\n  // XY is in (-1, 1) space, Z is in (0, 1) space\n  let posFromLight = scene.lightViewProjMatrix * model.modelMatrix * vec4(position, 1.0);\n\n  // Convert XY to (0, 1)\n  // Y is flipped because texture coords are Y-down.\n  output.shadowPos = vec3(\n    posFromLight.xy * vec2(0.5, -0.5) + vec2(0.5),\n    posFromLight.z\n  );\n\n  output.Position = scene.cameraViewProjMatrix * model.modelMatrix * vec4(position, 1.0);\n  output.fragPos = output.Position.xyz;\n  output.fragNorm = normal;\n  return output;\n}\n",u="override shadowDepthTextureSize: f32 = 1024.0;\n\nstruct Scene {\n  lightViewProjMatrix : mat4x4<f32>,\n  cameraViewProjMatrix : mat4x4<f32>,\n  lightPos : vec3<f32>,\n}\n\n@group(0) @binding(0) var<uniform> scene : Scene;\n@group(0) @binding(1) var shadowMap: texture_depth_2d;\n@group(0) @binding(2) var shadowSampler: sampler_comparison;\n\nstruct FragmentInput {\n  @location(0) shadowPos : vec3<f32>,\n  @location(1) fragPos : vec3<f32>,\n  @location(2) fragNorm : vec3<f32>,\n}\n\nconst albedo = vec3<f32>(0.9);\nconst ambientFactor = 0.2;\n\n@fragment\nfn main(input : FragmentInput) -> @location(0) vec4<f32> {\n  // Percentage-closer filtering. Sample texels in the region\n  // to smooth the result.\n  var visibility = 0.0;\n  let oneOverShadowDepthTextureSize = 1.0 / shadowDepthTextureSize;\n  for (var y = -1; y <= 1; y++) {\n    for (var x = -1; x <= 1; x++) {\n      let offset = vec2<f32>(vec2(x, y)) * oneOverShadowDepthTextureSize;\n\n      visibility += textureSampleCompare(\n        shadowMap, shadowSampler,\n        input.shadowPos.xy + offset, input.shadowPos.z - 0.007\n      );\n    }\n  }\n  visibility /= 9.0;\n\n  let lambertFactor = max(dot(normalize(scene.lightPos - input.fragPos), normalize(input.fragNorm)), 0.0);\n  let lightingFactor = min(ambientFactor + visibility * lambertFactor, 1.0);\n\n  return vec4(lightingFactor * albedo, 1.0);\n}\n",l="src/sample/shadowMapping/main.ts";let init=async e=>{let{canvas:n,pageState:t}=e,a=await navigator.gpu.requestAdapter(),l=await a.requestDevice();if(!t.active)return;let c=n.getContext("webgpu"),d=window.devicePixelRatio;n.width=n.clientWidth*d,n.height=n.clientHeight*d;let f=n.width/n.height,m=navigator.gpu.getPreferredCanvasFormat();c.configure({device:l,format:m,alphaMode:"premultiplied"});let p=l.createBuffer({size:6*i.W.positions.length*Float32Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.VERTEX,mappedAtCreation:!0});{let e=new Float32Array(p.getMappedRange());for(let n=0;n<i.W.positions.length;++n)e.set(i.W.positions[n],6*n),e.set(i.W.normals[n],6*n+3);p.unmap()}let h=3*i.W.triangles.length,g=l.createBuffer({size:h*Uint16Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.INDEX,mappedAtCreation:!0});{let e=new Uint16Array(g.getMappedRange());for(let n=0;n<i.W.triangles.length;++n)e.set(i.W.triangles[n],3*n);g.unmap()}let v=l.createTexture({size:[1024,1024,1],usage:GPUTextureUsage.RENDER_ATTACHMENT|GPUTextureUsage.TEXTURE_BINDING,format:"depth32float"}),x=v.createView(),P=[{arrayStride:6*Float32Array.BYTES_PER_ELEMENT,attributes:[{shaderLocation:0,offset:0,format:"float32x3"},{shaderLocation:1,offset:3*Float32Array.BYTES_PER_ELEMENT,format:"float32x3"}]}],w={topology:"triangle-list",cullMode:"back"},b=l.createBindGroupLayout({entries:[{binding:0,visibility:GPUShaderStage.VERTEX,buffer:{type:"uniform"}}]}),y=l.createRenderPipeline({layout:l.createPipelineLayout({bindGroupLayouts:[b,b]}),vertex:{module:l.createShaderModule({code:o}),entryPoint:"main",buffers:P},depthStencil:{depthWriteEnabled:!0,depthCompare:"less",format:"depth32float"},primitive:w}),S=l.createBindGroupLayout({entries:[{binding:0,visibility:GPUShaderStage.VERTEX|GPUShaderStage.FRAGMENT,buffer:{type:"uniform"}},{binding:1,visibility:GPUShaderStage.VERTEX|GPUShaderStage.FRAGMENT,texture:{sampleType:"depth"}},{binding:2,visibility:GPUShaderStage.VERTEX|GPUShaderStage.FRAGMENT,sampler:{type:"comparison"}}]}),E=l.createRenderPipeline({layout:l.createPipelineLayout({bindGroupLayouts:[S,b]}),vertex:{module:l.createShaderModule({code:s}),entryPoint:"main",buffers:P},fragment:{module:l.createShaderModule({code:u}),entryPoint:"main",targets:[{format:m}],constants:{shadowDepthTextureSize:1024}},depthStencil:{depthWriteEnabled:!0,depthCompare:"less",format:"depth24plus-stencil8"},primitive:w}),B=l.createTexture({size:[n.width,n.height],format:"depth24plus-stencil8",usage:GPUTextureUsage.RENDER_ATTACHMENT}),M={colorAttachments:[{view:void 0,clearValue:{r:.5,g:.5,b:.5,a:1},loadOp:"clear",storeOp:"store"}],depthStencilAttachment:{view:B.createView(),depthClearValue:1,depthLoadOp:"clear",depthStoreOp:"store",stencilClearValue:0,stencilLoadOp:"clear",stencilStoreOp:"store"}},G=l.createBuffer({size:64,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),T=l.createBuffer({size:144,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),R=l.createBindGroup({layout:b,entries:[{binding:0,resource:{buffer:T}}]}),U=l.createBindGroup({layout:S,entries:[{binding:0,resource:{buffer:T}},{binding:1,resource:x},{binding:2,resource:l.createSampler({compare:"less"})}]}),_=l.createBindGroup({layout:b,entries:[{binding:0,resource:{buffer:G}}]}),V=r.R3.fromValues(0,50,-100),L=r.R3.fromValues(0,1,0),C=r.R3.fromValues(0,0,0),A=r._E.perspective(2*Math.PI/5,f,1,2e3),F=r._E.lookAt(V,C,L),j=r.R3.fromValues(50,100,-100),D=r._E.lookAt(j,C,L),N=r._E.create();r._E.ortho(-80,80,-80,80,-200,300,N);let O=r._E.multiply(N,D),z=r._E.multiply(A,F),I=r._E.translation([0,-45,0]);l.queue.writeBuffer(T,0,O.buffer,O.byteOffset,O.byteLength),l.queue.writeBuffer(T,64,z.buffer,z.byteOffset,z.byteLength),l.queue.writeBuffer(T,128,j.buffer,j.byteOffset,j.byteLength),l.queue.writeBuffer(G,0,I.buffer,I.byteOffset,I.byteLength);let W={colorAttachments:[],depthStencilAttachment:{view:x,depthClearValue:1,depthLoadOp:"clear",depthStoreOp:"store"}};requestAnimationFrame(function frame(){if(!t.active)return;let e=function(){let e=r.R3.fromValues(0,50,-100),n=Math.PI*(Date.now()/2e3),t=r._E.rotateY(r._E.translation(C),n);r.R3.transformMat4(e,t,e);let a=r._E.lookAt(e,C,L);return r._E.multiply(A,a,z),z}();l.queue.writeBuffer(T,64,e.buffer,e.byteOffset,e.byteLength),M.colorAttachments[0].view=c.getCurrentTexture().createView();let n=l.createCommandEncoder();{let e=n.beginRenderPass(W);e.setPipeline(y),e.setBindGroup(0,R),e.setBindGroup(1,_),e.setVertexBuffer(0,p),e.setIndexBuffer(g,"uint16"),e.drawIndexed(h),e.end()}{let e=n.beginRenderPass(M);e.setPipeline(E),e.setBindGroup(0,U),e.setBindGroup(1,_),e.setVertexBuffer(0,p),e.setIndexBuffer(g,"uint16"),e.drawIndexed(h),e.end()}l.queue.submit([n.finish()]),requestAnimationFrame(frame)})};var main=()=>(0,a.Tl)({name:"Shadow Mapping",description:"This example shows how to sample from a depth texture to render shadows.",init,sources:[{name:l.substring(25),contents:"import { mat4, vec3 } from 'wgpu-matrix';\nimport { makeSample, SampleInit } from '../../components/SampleLayout';\n\nimport { mesh } from '../../meshes/stanfordDragon';\n\nimport vertexShadowWGSL from './vertexShadow.wgsl';\nimport vertexWGSL from './vertex.wgsl';\nimport fragmentWGSL from './fragment.wgsl';\n\nconst shadowDepthTextureSize = 1024;\n\nconst init: SampleInit = async ({ canvas, pageState }) => {\n  const adapter = await navigator.gpu.requestAdapter();\n  const device = await adapter.requestDevice();\n\n  if (!pageState.active) return;\n\n  const context = canvas.getContext('webgpu') as GPUCanvasContext;\n\n  const devicePixelRatio = window.devicePixelRatio;\n  canvas.width = canvas.clientWidth * devicePixelRatio;\n  canvas.height = canvas.clientHeight * devicePixelRatio;\n  const aspect = canvas.width / canvas.height;\n  const presentationFormat = navigator.gpu.getPreferredCanvasFormat();\n  context.configure({\n    device,\n    format: presentationFormat,\n    alphaMode: 'premultiplied',\n  });\n\n  // Create the model vertex buffer.\n  const vertexBuffer = device.createBuffer({\n    size: mesh.positions.length * 3 * 2 * Float32Array.BYTES_PER_ELEMENT,\n    usage: GPUBufferUsage.VERTEX,\n    mappedAtCreation: true,\n  });\n  {\n    const mapping = new Float32Array(vertexBuffer.getMappedRange());\n    for (let i = 0; i < mesh.positions.length; ++i) {\n      mapping.set(mesh.positions[i], 6 * i);\n      mapping.set(mesh.normals[i], 6 * i + 3);\n    }\n    vertexBuffer.unmap();\n  }\n\n  // Create the model index buffer.\n  const indexCount = mesh.triangles.length * 3;\n  const indexBuffer = device.createBuffer({\n    size: indexCount * Uint16Array.BYTES_PER_ELEMENT,\n    usage: GPUBufferUsage.INDEX,\n    mappedAtCreation: true,\n  });\n  {\n    const mapping = new Uint16Array(indexBuffer.getMappedRange());\n    for (let i = 0; i < mesh.triangles.length; ++i) {\n      mapping.set(mesh.triangles[i], 3 * i);\n    }\n    indexBuffer.unmap();\n  }\n\n  // Create the depth texture for rendering/sampling the shadow map.\n  const shadowDepthTexture = device.createTexture({\n    size: [shadowDepthTextureSize, shadowDepthTextureSize, 1],\n    usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.TEXTURE_BINDING,\n    format: 'depth32float',\n  });\n  const shadowDepthTextureView = shadowDepthTexture.createView();\n\n  // Create some common descriptors used for both the shadow pipeline\n  // and the color rendering pipeline.\n  const vertexBuffers: Iterable<GPUVertexBufferLayout> = [\n    {\n      arrayStride: Float32Array.BYTES_PER_ELEMENT * 6,\n      attributes: [\n        {\n          // position\n          shaderLocation: 0,\n          offset: 0,\n          format: 'float32x3',\n        },\n        {\n          // normal\n          shaderLocation: 1,\n          offset: Float32Array.BYTES_PER_ELEMENT * 3,\n          format: 'float32x3',\n        },\n      ],\n    },\n  ];\n\n  const primitive: GPUPrimitiveState = {\n    topology: 'triangle-list',\n    cullMode: 'back',\n  };\n\n  const uniformBufferBindGroupLayout = device.createBindGroupLayout({\n    entries: [\n      {\n        binding: 0,\n        visibility: GPUShaderStage.VERTEX,\n        buffer: {\n          type: 'uniform',\n        },\n      },\n    ],\n  });\n\n  const shadowPipeline = device.createRenderPipeline({\n    layout: device.createPipelineLayout({\n      bindGroupLayouts: [\n        uniformBufferBindGroupLayout,\n        uniformBufferBindGroupLayout,\n      ],\n    }),\n    vertex: {\n      module: device.createShaderModule({\n        code: vertexShadowWGSL,\n      }),\n      entryPoint: 'main',\n      buffers: vertexBuffers,\n    },\n    depthStencil: {\n      depthWriteEnabled: true,\n      depthCompare: 'less',\n      format: 'depth32float',\n    },\n    primitive,\n  });\n\n  // Create a bind group layout which holds the scene uniforms and\n  // the texture+sampler for depth. We create it manually because the WebPU\n  // implementation doesn't infer this from the shader (yet).\n  const bglForRender = device.createBindGroupLayout({\n    entries: [\n      {\n        binding: 0,\n        visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT,\n        buffer: {\n          type: 'uniform',\n        },\n      },\n      {\n        binding: 1,\n        visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT,\n        texture: {\n          sampleType: 'depth',\n        },\n      },\n      {\n        binding: 2,\n        visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT,\n        sampler: {\n          type: 'comparison',\n        },\n      },\n    ],\n  });\n\n  const pipeline = device.createRenderPipeline({\n    layout: device.createPipelineLayout({\n      bindGroupLayouts: [bglForRender, uniformBufferBindGroupLayout],\n    }),\n    vertex: {\n      module: device.createShaderModule({\n        code: vertexWGSL,\n      }),\n      entryPoint: 'main',\n      buffers: vertexBuffers,\n    },\n    fragment: {\n      module: device.createShaderModule({\n        code: fragmentWGSL,\n      }),\n      entryPoint: 'main',\n      targets: [\n        {\n          format: presentationFormat,\n        },\n      ],\n      constants: {\n        shadowDepthTextureSize,\n      },\n    },\n    depthStencil: {\n      depthWriteEnabled: true,\n      depthCompare: 'less',\n      format: 'depth24plus-stencil8',\n    },\n    primitive,\n  });\n\n  const depthTexture = device.createTexture({\n    size: [canvas.width, canvas.height],\n    format: 'depth24plus-stencil8',\n    usage: GPUTextureUsage.RENDER_ATTACHMENT,\n  });\n\n  const renderPassDescriptor: GPURenderPassDescriptor = {\n    colorAttachments: [\n      {\n        // view is acquired and set in render loop.\n        view: undefined,\n\n        clearValue: { r: 0.5, g: 0.5, b: 0.5, a: 1.0 },\n        loadOp: 'clear',\n        storeOp: 'store',\n      },\n    ],\n    depthStencilAttachment: {\n      view: depthTexture.createView(),\n\n      depthClearValue: 1.0,\n      depthLoadOp: 'clear',\n      depthStoreOp: 'store',\n      stencilClearValue: 0,\n      stencilLoadOp: 'clear',\n      stencilStoreOp: 'store',\n    },\n  };\n\n  const modelUniformBuffer = device.createBuffer({\n    size: 4 * 16, // 4x4 matrix\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n\n  const sceneUniformBuffer = device.createBuffer({\n    // Two 4x4 viewProj matrices,\n    // one for the camera and one for the light.\n    // Then a vec3 for the light position.\n    // Rounded to the nearest multiple of 16.\n    size: 2 * 4 * 16 + 4 * 4,\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n\n  const sceneBindGroupForShadow = device.createBindGroup({\n    layout: uniformBufferBindGroupLayout,\n    entries: [\n      {\n        binding: 0,\n        resource: {\n          buffer: sceneUniformBuffer,\n        },\n      },\n    ],\n  });\n\n  const sceneBindGroupForRender = device.createBindGroup({\n    layout: bglForRender,\n    entries: [\n      {\n        binding: 0,\n        resource: {\n          buffer: sceneUniformBuffer,\n        },\n      },\n      {\n        binding: 1,\n        resource: shadowDepthTextureView,\n      },\n      {\n        binding: 2,\n        resource: device.createSampler({\n          compare: 'less',\n        }),\n      },\n    ],\n  });\n\n  const modelBindGroup = device.createBindGroup({\n    layout: uniformBufferBindGroupLayout,\n    entries: [\n      {\n        binding: 0,\n        resource: {\n          buffer: modelUniformBuffer,\n        },\n      },\n    ],\n  });\n\n  const eyePosition = vec3.fromValues(0, 50, -100);\n  const upVector = vec3.fromValues(0, 1, 0);\n  const origin = vec3.fromValues(0, 0, 0);\n\n  const projectionMatrix = mat4.perspective(\n    (2 * Math.PI) / 5,\n    aspect,\n    1,\n    2000.0\n  );\n\n  const viewMatrix = mat4.lookAt(eyePosition, origin, upVector);\n\n  const lightPosition = vec3.fromValues(50, 100, -100);\n  const lightViewMatrix = mat4.lookAt(lightPosition, origin, upVector);\n  const lightProjectionMatrix = mat4.create();\n  {\n    const left = -80;\n    const right = 80;\n    const bottom = -80;\n    const top = 80;\n    const near = -200;\n    const far = 300;\n    mat4.ortho(left, right, bottom, top, near, far, lightProjectionMatrix);\n  }\n\n  const lightViewProjMatrix = mat4.multiply(\n    lightProjectionMatrix,\n    lightViewMatrix\n  );\n\n  const viewProjMatrix = mat4.multiply(projectionMatrix, viewMatrix);\n\n  // Move the model so it's centered.\n  const modelMatrix = mat4.translation([0, -45, 0]);\n\n  // The camera/light aren't moving, so write them into buffers now.\n  {\n    const lightMatrixData = lightViewProjMatrix as Float32Array;\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      0,\n      lightMatrixData.buffer,\n      lightMatrixData.byteOffset,\n      lightMatrixData.byteLength\n    );\n\n    const cameraMatrixData = viewProjMatrix as Float32Array;\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      64,\n      cameraMatrixData.buffer,\n      cameraMatrixData.byteOffset,\n      cameraMatrixData.byteLength\n    );\n\n    const lightData = lightPosition as Float32Array;\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      128,\n      lightData.buffer,\n      lightData.byteOffset,\n      lightData.byteLength\n    );\n\n    const modelData = modelMatrix as Float32Array;\n    device.queue.writeBuffer(\n      modelUniformBuffer,\n      0,\n      modelData.buffer,\n      modelData.byteOffset,\n      modelData.byteLength\n    );\n  }\n\n  // Rotates the camera around the origin based on time.\n  function getCameraViewProjMatrix() {\n    const eyePosition = vec3.fromValues(0, 50, -100);\n\n    const rad = Math.PI * (Date.now() / 2000);\n    const rotation = mat4.rotateY(mat4.translation(origin), rad);\n    vec3.transformMat4(eyePosition, rotation, eyePosition);\n\n    const viewMatrix = mat4.lookAt(eyePosition, origin, upVector);\n\n    mat4.multiply(projectionMatrix, viewMatrix, viewProjMatrix);\n    return viewProjMatrix as Float32Array;\n  }\n\n  const shadowPassDescriptor: GPURenderPassDescriptor = {\n    colorAttachments: [],\n    depthStencilAttachment: {\n      view: shadowDepthTextureView,\n      depthClearValue: 1.0,\n      depthLoadOp: 'clear',\n      depthStoreOp: 'store',\n    },\n  };\n\n  function frame() {\n    // Sample is no longer the active page.\n    if (!pageState.active) return;\n\n    const cameraViewProj = getCameraViewProjMatrix();\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      64,\n      cameraViewProj.buffer,\n      cameraViewProj.byteOffset,\n      cameraViewProj.byteLength\n    );\n\n    renderPassDescriptor.colorAttachments[0].view = context\n      .getCurrentTexture()\n      .createView();\n\n    const commandEncoder = device.createCommandEncoder();\n    {\n      const shadowPass = commandEncoder.beginRenderPass(shadowPassDescriptor);\n      shadowPass.setPipeline(shadowPipeline);\n      shadowPass.setBindGroup(0, sceneBindGroupForShadow);\n      shadowPass.setBindGroup(1, modelBindGroup);\n      shadowPass.setVertexBuffer(0, vertexBuffer);\n      shadowPass.setIndexBuffer(indexBuffer, 'uint16');\n      shadowPass.drawIndexed(indexCount);\n\n      shadowPass.end();\n    }\n    {\n      const renderPass = commandEncoder.beginRenderPass(renderPassDescriptor);\n      renderPass.setPipeline(pipeline);\n      renderPass.setBindGroup(0, sceneBindGroupForRender);\n      renderPass.setBindGroup(1, modelBindGroup);\n      renderPass.setVertexBuffer(0, vertexBuffer);\n      renderPass.setIndexBuffer(indexBuffer, 'uint16');\n      renderPass.drawIndexed(indexCount);\n\n      renderPass.end();\n    }\n    device.queue.submit([commandEncoder.finish()]);\n    requestAnimationFrame(frame);\n  }\n  requestAnimationFrame(frame);\n};\n\nconst ShadowMapping: () => JSX.Element = () =>\n  makeSample({\n    name: 'Shadow Mapping',\n    description:\n      'This example shows how to sample from a depth texture to render shadows.',\n    init,\n    sources: [\n      {\n        name: __filename.substring(__dirname.length + 1),\n        contents: __SOURCE__,\n      },\n      {\n        name: './vertexShadow.wgsl',\n        contents: vertexShadowWGSL,\n        editable: true,\n      },\n      {\n        name: './vertex.wgsl',\n        contents: vertexWGSL,\n        editable: true,\n      },\n      {\n        name: './fragment.wgsl',\n        contents: fragmentWGSL,\n        editable: true,\n      },\n    ],\n    filename: __filename,\n  });\n\nexport default ShadowMapping;\n"},{name:"./vertexShadow.wgsl",contents:o,editable:!0},{name:"./vertex.wgsl",contents:s,editable:!0},{name:"./fragment.wgsl",contents:u,editable:!0}],filename:l})},4131:function(e){e.exports={canvasContainer:"SampleLayout_canvasContainer__ZTWP5",sourceFileNav:"SampleLayout_sourceFileNav__9Hf73",sourceFileContainer:"SampleLayout_sourceFileContainer__9iti6"}}}]);